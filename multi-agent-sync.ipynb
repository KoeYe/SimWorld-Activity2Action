{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UE5 Multi-Agent x LLM Reasoners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shadow\\miniconda3\\envs\\citynav\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "import time\n",
    "import threading\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "from dotenv import load_dotenv\n",
    "from A2A.agent import A2Agent\n",
    "from UE.unrealcv_a2a import UnrealCvA2A\n",
    "from UE.unrealcv_basic import UnrealCV\n",
    "# from ue_agent.agent import UEAgent\n",
    "# from ue_agent.plan_agent import PLAgent\n",
    "from llm.openai_model import UEOpenAIModel\n",
    "# from ue_agent.base import ActionBuffer\n",
    "from Prompt.prompt import USER_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from reasoners.lm.openai_model import OpenAIModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = OpenAIModel(\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.generate(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__init__:230:Got connection confirm: b'connected to gym_citynav'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=>Info: using ip-port socket\n"
     ]
    }
   ],
   "source": [
    "unrealcv_client_actor = UnrealCvA2A(port=9000, ip='127.0.0.1', resolution=(1280, 960))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unrealcv_client_observer = UnrealCV(port=9000, ip='127.0.0.1', resolution=(1280, 960))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BP_SpotRobot_C_1\n",
      "BP_SpotRobot_C_2\n",
      "BP_Default_Character_C_2\n"
     ]
    }
   ],
   "source": [
    "objects = unrealcv_client_actor.get_objects()\n",
    "for obj in objects:\n",
    "    if 'robot' in str(obj).lower() or 'character' in str(obj).lower():\n",
    "        print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objects = unrealcv_client_observer.get_objects()\n",
    "# for obj in objects:\n",
    "#     if 'robot' in str(obj).lower() or 'character' in str(obj).lower():\n",
    "#         print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unrealcv_client_actor.enable_controller('BP_Default_Character_C_2', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unrealcv_client_actor.apply_action_transition('BP_Default_Character_C_2', [100, 10 , 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UEOpenAIModel(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize the agent\n",
    "# target = \"Red Cube\"\n",
    "# goal = f\"\"\"Approach the {target}\n",
    "# if the {target} is not in the current observation, find it by rotating the robot to looking around\n",
    "# then make sure the {target} is at the center of the robot's field of view\n",
    "# \"\"\"\n",
    "\n",
    "# des_user_prompt1 = f\"\"\"\n",
    "#     According to the current observations images, describe the environment in detail.\n",
    "#     Your description should include the following information:\n",
    "#     - Need to explore:Whether the {target} is in the field of view.\n",
    "#     - Direction:The **relative** direction of the {target} to the robot (left side means robot needs to turn left, right side means robot needs to turn right).\n",
    "#     - Distance:If the **relative** direction of the {target} to the robot is correct, calculate the distance to the {target} and move to it.\n",
    "#     - Suggestion:Suggest a high level plan on how to get to the {target}, the actions to achieve the {target} can be rotate, move, etc.(Include the specific degree description, such as slight, medium, large, as well as how to move to the object etc.)\"\n",
    "#     just key information, plain text, no other information.\n",
    "# \"\"\"\n",
    "\n",
    "# des_user_prompt = f\"\"\" You are a vision-language model (VLM) agent. Based on your current visual observations, determine whether the target associated with the following goal is visible in your field of view: {goal}.\n",
    "# Below is an example to illustrate how you should respond:\n",
    "# Example Goal: “Navigate to the red mailbox.”\n",
    "# Example Response:\n",
    "# - Visibility: Yes, the red mailbox is visible.\n",
    "# - High-Level Plan:\n",
    "#   - Move forward 5 meters toward the mailbox.\n",
    "#   - Adjust direction slightly to the right by 15 degrees to align directly with the mailbox.\n",
    "# Note: The lower-level language model (LLM) does not have visual access to the environment. Therefore, you must clearly determine target visibility and provide precise, actionable instructions.\n",
    "# Now, using the example above, generate a similarly structured high-level plan to accomplish the current goal. \"\"\"\n",
    "\n",
    "# des_system_prompt1 =  f\"\"\"\n",
    "#     You are a robot named <NAME> in a city.\n",
    "#     You have the observation of the environment, which is a list of different types of images of the environment,\n",
    "#     including lit, normal, depth, and object_mask images.\n",
    "#     Your goal is describe the environment according to your observations in detail and give the high level plan to achieve the goal:{goal} according to the description and history of actions.\n",
    "#     Do NOT repeat the same type of action consecutively. Such as rotate left and rotate right back again.\n",
    "#     Move forward when the action history is full of rotate actions.\n",
    "# \"\"\"\n",
    "\n",
    "# des_system_prompt =  f\"\"\"\n",
    "#     You are a robot named <NAME> in a city.\n",
    "#     You have the observation of the environment, which is a list of different images of the environment,\n",
    "#     Your goal is to give the high level plan to achieve the goal:{goal} according to the description and history of actions.\n",
    "#     Do NOT repeat the same type of action consecutively. Such as rotate left and rotate right back again.\n",
    "#     Move forward when the action history is full of rotate actions.\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"The suggest action for next step to achieve the {target} (rotate, move, etc.). (Include the specific degree description, such as slight, medium, large, as well as how to move to the object etc.)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_observation(unrealcv_client, cam_id, viewmode, observation_space, description_space, agent):\n",
    "#     for id in cam_id:\n",
    "#         img = unrealcv_client.read_image(id, viewmode)\n",
    "#         observation_space[id] = img\n",
    "#         description_space[id] = agent.describe([img], system_prompt=des_user_prompt, user_prompt=des_system_prompt).text[0]\n",
    "#     # with open(\"description.txt\", \"a\") as f:\n",
    "#     #     f.write(f\"{cam_id}:\\n\")\n",
    "#     #     f.write(f\"    {description_space[id]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def take_action(agent: UEAgent, pagent: PLAgent):\n",
    "#     # print(\"calling action\", flush=True)\n",
    "#     action_step = 0\n",
    "#     while action_step <= 50:\n",
    "#         camera_id = agent.camera_id\n",
    "#         try:\n",
    "            \n",
    "#             observation = agent.client.read_image(camera_id, \"lit\", mode=\"file\") # block\n",
    "#             print(\"finsh reading\", flush=True)\n",
    "#             description = agent.describe([observation],system_prompt=des_user_prompt, user_prompt=des_system_prompt).text[0] # block\n",
    "#             # print(description, flush=True)\n",
    "#             print(\"finish_ oberved\", flush=True)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"error {e}\", flush=True)\n",
    "#             print(\"there\", flush=True)\n",
    "#             # continue\n",
    "#         steps = pagent.act(observation, description=description)\n",
    "#         action_step += steps\n",
    "#         print(f\"steps{action_step}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation=Agent_Human.client.read_image(1, \"depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent_Human.describe([observation], system_prompt=des_user_prompt, user_prompt=des_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_prompt = USER_PROMPT.format(waypoint=\"(4,7)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    Agent_Human = A2Agent(\n",
    "        model=model,\n",
    "        unrealcv_client=unrealcv_client_actor,\n",
    "        name=\"BP_Default_Character_C_2\",\n",
    "        temperature=1.5,\n",
    "        max_history_step=5,\n",
    "        camera_id=1,\n",
    "        observation_viewmode='lit',\n",
    "        # action_buffer=action_buffer\n",
    "    )\n",
    "    Agent_Dog_1 = A2Agent(\n",
    "        model=model,\n",
    "        unrealcv_client=unrealcv_client_actor,\n",
    "        name=\"BP_SpotRobot_C_1\",\n",
    "        camera_id=3,\n",
    "        temperature=1.5,\n",
    "        max_history_step=5,\n",
    "        observation_viewmode='lit',\n",
    "        # action_buffer=action_buffer\n",
    "    )\n",
    "    Agent_Dog_2 = A2Agent(\n",
    "        model=model,\n",
    "        unrealcv_client=unrealcv_client_actor,\n",
    "        name=\"BP_SpotRobot_C_2\",\n",
    "        camera_id=2,\n",
    "        temperature=1.5,\n",
    "        max_history_step=5,\n",
    "        observation_viewmode='lit',\n",
    "        # action_buffer=action_buffer\n",
    "    )\n",
    "\n",
    "    Agent_Human.reset()\n",
    "    Agent_Dog_1.reset()\n",
    "    Agent_Dog_2.reset()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {\"actions\":[0,0],\"waypoints\":{\"x\":4,\"y\":7}}\n",
      "Next waypoint: Vector(x=4.0, y=7.0)\n",
      "Next waypoint: Vector(x=4.0, y=7.0)\n"
     ]
    }
   ],
   "source": [
    "Agent_Dog_1.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take_action(Agent_Human, observation_space, description_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Agent_Dog_1.client.read_image(0, \"depth\", mode=\"direct\")\n",
    "# image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citynav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
